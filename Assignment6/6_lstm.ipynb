{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298195 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "xwnxantqyolu w ze md aenglxextkx c oau sjnxtbnbaopnrelbodukntexpips  cjnihd urec\n",
      "vxwabkicazsgstq qy aoqosrnt oenpf v ku s mnniae maakhni  khprclwimahjawu eqfpi  \n",
      "fgxnte kenktau tl bv  emn lheynhveeagaovnan txtyiqgbfuu  k wi qtnaaait t xnw fle\n",
      "ch tneti zsgbnor gndflhctk ma  berxxgykqtmvrbz ekssq tnvpyh g nhkr nt p  tdoquki\n",
      " se  enatedcezugyzaegd pun siicvtfxiutktktcfp t izh  dbeez jc   azmif wwopj xqkr\n",
      "================================================================================\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 100: 2.600748 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.78\n",
      "Validation set perplexity: 10.15\n",
      "Average loss at step 200: 2.233246 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.63\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 300: 2.098580 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 2.001117 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 500: 1.938334 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 600: 1.907715 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 700: 1.859718 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800: 1.817983 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 900: 1.827957 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1000: 1.826514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "zers desulop its bean  wam hild deach an spand inmaaliture be andrenman host to \n",
      "es the lise of indise supsiof gooz conol and aadents toix in groces the incade c\n",
      "or of the ofpen not and ic noo providition the and seven s envected b consmpured\n",
      "h stated rebwensicks wile reteric kater frenviess edool vezwer eqpused igl an ea\n",
      "us of the depasy devole he one nine zero seven poved the recommment in one will \n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100: 1.773399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200: 1.746822 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1300: 1.729670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1400: 1.744618 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1500: 1.731719 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1600: 1.742318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1700: 1.702278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.672797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.643750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2000: 1.692289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "x courcas in highorngaust a this persuilan one nine five zero zero hown soxth ni\n",
      "hor wingosvedpod sevenc as lafe contrisser at shorths previnoup hoimp lalgest ge\n",
      "quess the transitics as chouction bisk ima dakinest sadrive tenculard vouthateou\n",
      "nians a dereatle in becpesaci minoreay the linguctilancemp dont simply asips now\n",
      "m that the champras christ as a to cospusion prostarion brouphed formand and nin\n",
      "================================================================================\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2100: 1.682217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2200: 1.678105 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2300: 1.642155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.660834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2500: 1.676779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2600: 1.655910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2700: 1.655609 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2800: 1.651162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2900: 1.650775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3000: 1.648502 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "prow bous the roulded maded firmon prabeld festempper is gelables consolist of r\n",
      "t with one vichnstic which landsbre is note zeron the willint jale a pronouncd a\n",
      "giabliare was lasistical selatles on water nationally kimon resulory in five of \n",
      "pucific they lardhicine county bath surrlider chithall y in the loots and a regi\n",
      "guan piolly the tranded president adact backows aurlands s liars the courding ca\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3100: 1.626272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3200: 1.645855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3300: 1.640904 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3400: 1.667181 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3500: 1.658155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.668498 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3700: 1.645122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3800: 1.645248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3900: 1.633490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.648455 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "ann hotas the cablic depant cappuies compist chiles curration hive five normed m\n",
      "es airsitys fount five peoplaty in the the eventer geneated into sociey amividy \n",
      "pre factal and that his rid convoduress formical programs music world of the att\n",
      "ner one two zero zero three indian comphide cult the normal person albect cemmed\n",
      "chiph ab capreaall in mis orwincial kamay becancerfictarly the recembers such of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4100: 1.633019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4200: 1.632651 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4300: 1.614795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4400: 1.606969 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4500: 1.615374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4600: 1.613898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.625823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.627481 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4900: 1.632303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5000: 1.602712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "z a presentipic vioted that could the deciduentic backes of chastices is trealis\n",
      "zagel within in the smapt one one nine nine nine seven one seven two zero zero d\n",
      "loms tolen the did sapatine indication pracess bo god tritishs jimister all crie\n",
      "wige callanginal eight nine zero creved aquical had borkive in the remany bool g\n",
      "icallity flees  biok two return the a musto forcess as waves beriverryage queaj \n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5100: 1.603539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5200: 1.586011 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5300: 1.576432 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.578954 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5500: 1.565003 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5600: 1.582207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5700: 1.569549 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5800: 1.579960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5900: 1.572173 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6000: 1.543684 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "y southee but obteckation and presidence unix was sole convent sm lawily local w\n",
      "gually rofement cape the was the reconded stanricle deather de founder from over\n",
      "forwardrial castooge convele pass in convention we jaz olia pegalia lataris spec\n",
      "standing one nine eight edthome state is news planes to de agound and florids en\n",
      "oniaw gata than forcebaria are her a t wakere all culebummination underwint agab\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6100: 1.564796 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6200: 1.534572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6300: 1.546437 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6400: 1.540572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.555252 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.596032 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6700: 1.578373 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800: 1.603082 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6900: 1.581530 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.577532 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "factive if s relosm acciples most possocetics includingnion as the high at the b\n",
      "re one four nine mane largely memmers this grough a lows position an indorred fa\n",
      "d of the abieral hag player in four five three incrossel orgho of the one four t\n",
      "xt is united women celveror wold shakeps by the continoover gogiar from the brit\n",
      "ned as for no more an b convilusler a reserreanshically interrect nine the wille\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters  \n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(1, 4, smatmul)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294645 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "uwoebz avjumrrm x y kluzekftzulknl   mwszobmr r  cgjvtnccmjhqokpxlcoyife  lvegon\n",
      "ye  xgijttf szpttesglrwttirn gdzsfo uefr y hwzx hw gwf  elzaemctlruociaqnoudioy \n",
      "ootmedaiffpaan chkqqdboe slzabfshpg fcffrel nin syrac veuxtrwdxnxp uemhjouwgsbrx\n",
      "xdldrtrnrhehbohpkwfmesfj elpx sfanqj rqrdqnwj fx psnesxraledqevewpqapthdqdtejs x\n",
      "ffeqnutm cimiyhnvrf wuakihgajnsrjm yh   b xhft zyeeetnznhupojcct usbjuvlnrzpthhr\n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.591863 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.64\n",
      "Validation set perplexity: 10.11\n",
      "Average loss at step 200: 2.247835 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.62\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 300: 2.096182 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 400: 1.995423 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 500: 1.932388 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 600: 1.904832 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.855498 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.814458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 900: 1.828062 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1000: 1.823390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "fory and nayall was peittion celtwh nearcher remobed of fealuece unsic follo and\n",
      "manu ivean in he by nine betnoton to puefing is and hesaces upsoral incuerea one\n",
      "jubar oritan form in lower three beforegity of sechetique ageration of equel to \n",
      "gent of enares hele comdues and and versands of gret one one disy with string sy\n",
      "s tee of prentian the greacund obeer nirzer one four nine excoanst y de resext m\n",
      "================================================================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1100: 1.774175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1200: 1.749117 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1300: 1.733567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.739998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1500: 1.732100 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1600: 1.744401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1700: 1.709431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1800: 1.670360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 1900: 1.643897 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2000: 1.693717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "rand alsowes altero historcians beeinal lin the epherarm styly opers as nomse th\n",
      "mus on postifich on i a frum moun toxceration with four chnoplingminien worlf ch\n",
      "f the grash mingenso in the moral way main as remo aflecfong sed wha crean in th\n",
      "ficions in sleo ares concists and was loney havernocka assand hounded prood nali\n",
      "brecricchatly comment and be languaie prove four matters as numbers is three eig\n",
      "================================================================================\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2100: 1.679910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2200: 1.682342 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.637059 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2400: 1.657281 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2500: 1.680429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2600: 1.650990 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2700: 1.656200 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2800: 1.648829 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2900: 1.649218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3000: 1.652380 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "logicel kands traffo offices passed in indlarscking traised bothers used bland b\n",
      "visin formspose works paptical spacenogal marietcally calgerings one nine norm o\n",
      "mentis the coldiction cormisty of the track with and the pasic in ball t aptroye\n",
      "st decnieg state and libolow pisciclinds eureplard it in seven six ninotical sta\n",
      "ver one eight six if deceired in maw collegion passity paseical the expertac nep\n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3100: 1.628764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3200: 1.646415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3300: 1.636887 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3400: 1.666566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3500: 1.652552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3600: 1.669073 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3700: 1.645765 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3800: 1.641019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3900: 1.635365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4000: 1.652260 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "ha gromals the perrenting smminino out the proposuis written bill velitives of g\n",
      "king the yaincted in for vet the bectien good as the kittorius degree calessiv b\n",
      "juted itmalve low code chicts this spribes tydent have lave expinite ucing genee\n",
      "jonecrities and genred jounk frompnold yhthectinhs adrikation list abcelt inctie\n",
      "i of officials the vama impendiste is it his visdest concerpate flocan they majo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4100: 1.632484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4200: 1.633243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4300: 1.613154 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4400: 1.609514 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4500: 1.612717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4600: 1.612674 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4700: 1.627838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4800: 1.625841 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4900: 1.635075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5000: 1.607124 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "pen empetualed strate enpoyd was varigent by jufel mext himel disispous abivia a\n",
      "zer hancin as two zero zero zero zero zero zero zero zero s from marinated and w\n",
      "x or in the s other is word to eaceed the design occusian the dot lounitial lous\n",
      "ts norwile having has dumitary of often the naty in the one or can invaving for \n",
      "inct graniting ili ko one zero oved meoul his mamer ne atchinites just bulish me\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5100: 1.602710 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5200: 1.587569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5300: 1.578854 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5400: 1.578724 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5500: 1.568017 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5600: 1.578881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5700: 1.568820 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5800: 1.581972 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5900: 1.573778 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.544186 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "fici women one one piputhrel ruch is soulabets or grammnad accountment attemples\n",
      "rocod and in consitions wwionze frong off four thrue imblosbanda s common to pol\n",
      "z by magne mater which sbind and lost zero two beson rolas comminion of settlent\n",
      "zer elected he cape caoliniston pibles and out b of wile bo iffect of not the bi\n",
      "y was although adam that cital yeargetisisa interporther modern agreed to the st\n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6100: 1.562585 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6200: 1.535449 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6300: 1.541613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6400: 1.535935 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6500: 1.554285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6600: 1.597516 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6700: 1.577869 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6800: 1.600783 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6900: 1.579061 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7000: 1.575352 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "genet one nine six two ecoive funda interned then three forming of muchoboruntin\n",
      "ys of the scholo generation rease draphoport jury that its hows is clayspand ass\n",
      "k the le rayi will acrixion is australia region descendaring gogit that six scor\n",
      "ult sacuations and commanding pumatics would abved in decland maduav cssetwabini\n",
      "ding ctoblisharger and sabanch s schandhantan where steaded propreter campar goo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first adapt the LSTM for a single character input with embeddings. The feed_dict is unchanged, the embeddings are looked up from the inputs. The output is an array probability for the possible characters, not an embedding, thus we use softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.303693 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.21\n",
      "================================================================================\n",
      "eei pxcbtropbgchbz sjve pna uhtoc   ibjaia rmel itelste tftpmorgx  tpileeesmew z\n",
      "wxue d te t neriinrmerh tfvueazyqrat c rdjnwherd y e y ttox dyaefvbpceevd bit a \n",
      "rixaehw et  gc mm ohcr  juo aaedmwzt dise geakd  t iefvitmrhta a a  teya ektuera\n",
      "s g l u mqu tc atilfl r dusae fksth kdd   omwglemvr  d sfs le zmh pz  l x pgd oe\n",
      "gtue tvwzgu t mtnez atecisoqj  gton eawisetcfzis tsin l rkel  aara e  eh sfgd hr\n",
      "================================================================================\n",
      "Validation set perplexity: 19.16\n",
      "Average loss at step 100: 2.277549 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.42\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 200: 2.007393 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 300: 1.929089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 400: 1.896447 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 500: 1.864504 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 600: 1.794303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 700: 1.771260 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 800: 1.789005 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 900: 1.770213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1000: 1.779255 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "n ubl apphicanent synnhat weppont of comptiore in one nine five poter the come e\n",
      "d one nine up of to his inearsh gamis coode recouks overning os the herat syst t\n",
      "sivel the uk strics from s migre willed of sampholic l weor only the portain mar\n",
      "groph twour sdference such the limary to i ra streptur in lifian havia is used o\n",
      "es of was and the named posed ton be vai like havking retriber incure three five\n",
      "================================================================================\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1100: 1.745233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1200: 1.710484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1300: 1.708081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1400: 1.719341 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1500: 1.703182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1600: 1.695689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1700: 1.681933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 1800: 1.657880 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 1900: 1.665523 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2000: 1.657596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "ger ansabellos extentias is neceate fry debaly was in the plans althought the se\n",
      "fered of foct resul a for a cusenes pebilly are sample nation firsse of that a p\n",
      "jecticklar suggrina delt a midepe thelp excespuss the sarponser it in bear sun a\n",
      " that german him in its neclance flitencotary tistrol lattern for us is syste an\n",
      "wnence he branguall and from deterrationy over eight clause of power persalian c\n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2100: 1.663412 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2200: 1.686035 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2300: 1.686071 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2400: 1.674320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2500: 1.673793 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2600: 1.664022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2700: 1.673338 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2800: 1.676214 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2900: 1.666866 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3000: 1.674388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "e exprestific new compensed to two three two two of the bobon high topachy which\n",
      "us new saveyss the fruan iri micharilans a pindinned kdery betw be deesto to the\n",
      "vious of the brittz the operon but to is missimate cords with turnial parished t\n",
      "qu it become britio of the pister white she parts the sladorgal deseasoposs for \n",
      "que beetwa perking of be two missimate the have of the buodin other t d general \n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3100: 1.646055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3200: 1.633672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3300: 1.642557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3400: 1.632331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3500: 1.667655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3600: 1.651745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3700: 1.652626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3800: 1.657170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3900: 1.656521 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4000: 1.645131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "z storct proces amering the triedure memble of is prists and natins we fulle fer\n",
      "legard misle has grontans of feix of feeurons atdowly and cusst radia of the min\n",
      "have radaints time itstria britt of hleprefdwall amourts and dequerate and show \n",
      "was s fellow be commentine it strective legle of liver for action hold and first\n",
      "f battive procies plates whho sogame hare not homalist miks to black both bamals\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4100: 1.631156 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4200: 1.619941 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4300: 1.626973 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4400: 1.620327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4500: 1.647376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4600: 1.629138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4700: 1.627830 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4800: 1.621966 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4900: 1.632959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5000: 1.633085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "ne be zery two zero two jewhards of it for mritent one nine five meonapphented o\n",
      "partail comparounces then bands of the shav lefbrogthly two fiverent compon very\n",
      "ke in peuku geters of the one seven which haldves sty like i in schiquen smarron\n",
      "bork carbologids rogings of wughloact star launds plate specumes for levest appi\n",
      "men after killhity as incradius the legit inegranged of inver nota englia state \n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5100: 1.582725 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5200: 1.577845 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5300: 1.581075 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5400: 1.572955 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5500: 1.570335 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5600: 1.544137 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5700: 1.556436 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5800: 1.581447 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5900: 1.563537 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6000: 1.563003 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "y his inventing the guiver and visis date boing by had woman faque econsitioned \n",
      " causes again was territories disnant harmop name see bu decearsam drocation tha\n",
      "wife of morplar americal each pouse impire examry knanovers suw with lando its a\n",
      "ark sconciunt other supporture eight and move two the his playing and member was\n",
      "ed all ons examust spounce the lending musicoler the khaill impay of the tradita\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6100: 1.558070 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6200: 1.566228 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6300: 1.560814 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6400: 1.554231 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6500: 1.537878 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6600: 1.578433 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6700: 1.548584 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6800: 1.553772 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6900: 1.550166 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7000: 1.568867 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "xic has halds bound whoory sacities overigina as acrex in suggest for compan ter\n",
      "k swisged whetherote musa by componism germats he isbn zero zero three eight two\n",
      "norang governming of bassies be cantures georpa heromens be his atsures of five \n",
      "th the end now for declayition is loaj agails apperal results was chinese the se\n",
      "ham yed to way undernal in was the states is wil and claimed homunis and muswart\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use bigrams as inputs for the training. Here again, the feed_dict is unchanged, the bigram embeddings are looked up from the inputs. The output of the LSTM is still a probability array of the possible characters (not bigrams). Notice we use batch generator to get a bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #print(i.get_shape())\n",
    "    #print(i)\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    #print(logits.get_shape())\n",
    "    #print(tf.concat(0, train_labels).get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.303107 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.20\n",
      "================================================================================\n",
      "v ek hd te xcpze ddj xqnz nvr cncsaxmnjzfs efpws zvfbxehtjsqy doe nvtvixgo pgmj z\n",
      "byxkez odtr   ebfarrunb njrt l hewzap wezheshpbtvypkfirkrg rajii v   cosfcaqksfqw\n",
      "lpw     dpvhmikkg fperfo peiikebjql v  ew yj ry eeznnkantqnud k ntta ygtrwt ncd a\n",
      "ik ngsncbghzbz mb lz w onenyw nnbaizhxermihfqlhrs p   igv  dnw   pmsehnbes r vmr \n",
      "uslcpvewzee kpbb twozk ce gvjjweiwekfkobrpioiu w adeev rag lt i scfsvimhnvgs a s \n",
      "================================================================================\n",
      "Validation set perplexity: 19.55\n",
      "Average loss at step 100: 2.289419 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.35\n",
      "Validation set perplexity: 9.15\n",
      "Average loss at step 200: 1.955552 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 300: 1.852153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 400: 1.802510 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 500: 1.821413 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 600: 1.763938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 700: 1.741662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 800: 1.740378 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 900: 1.744939 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 1000: 1.674441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "ychine enestrlyncer thaph stainal the hist migue of galt republish many which psi\n",
      "xcept is clists though to mind citic cyaned productions souryth difcs from a the \n",
      "gnmnessno one nine five one stated wing gps parack wing new year zero is formemen\n",
      "ican ace becaphir who the jampayed againden islimit tbilland of from set oracc jo\n",
      "gfee nine five one seve five it will orgeort cp rank from phey chintently rive is\n",
      "================================================================================\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 1100: 1.662800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 1200: 1.697390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 1300: 1.674699 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 1400: 1.652857 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 1500: 1.654828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 1600: 1.654948 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 1700: 1.677370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 1800: 1.651575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 1900: 1.654664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 2000: 1.670206 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "qytrial houghs aditainev octon two the phared wanisis of exheers scasous parretic\n",
      "rms nations magappring flight is interrebutest and norl the ready of three tread \n",
      "rplles starthuanatatures a sagenous when the warld from a cretem skillety antaria\n",
      "cdhg a now and pain kansee ba malms also being when a mtchssors t gifian local th\n",
      "gc is ences shortugant of kksis one nine robasand the ea mithenerahy n becricatio\n",
      "================================================================================\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 2100: 1.656386 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 2200: 1.636332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 2300: 1.638178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 2400: 1.644861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 2500: 1.673259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 2600: 1.644677 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 2700: 1.665440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2800: 1.621218 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 2900: 1.625497 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 3000: 1.634249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "xvoxide drespology line conpuencii of the collimans froma beforeed to air of dire\n",
      "rustial who zero two two dewisic hands of bent upportric resex imitation expznple\n",
      "conment the stay with densix seven nine many bellans arvice as called will derate\n",
      "sn pc of the empire of two miney in casion nor desus is a sample of choulists spr\n",
      "zz the it of the major of the ddries legaindon four one fivistication and spany f\n",
      "================================================================================\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 3100: 1.630916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 3200: 1.636494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3300: 1.612517 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3400: 1.618676 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 3500: 1.614961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 3600: 1.610917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3700: 1.611703 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 3800: 1.607865 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3900: 1.601111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 4000: 1.612437 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "ls innimational formater cusializon of the obasquely recorday of the style vol an\n",
      "imon war zero yes made were memblimselling s stribally six nine zero one mination\n",
      "aq germac one six birdln classwithm kint word his commonsorship of the plained ir\n",
      "ege tuws with maphyyals teamptians res would she because kazintwors to popular of\n",
      "fknavior km to ranwarvsure in the sacy band as the softwern in the west s sciousn\n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4100: 1.608217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 4200: 1.595290 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 4300: 1.581961 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 4400: 1.610497 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 4500: 1.616521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 4600: 1.619503 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 4700: 1.593652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 4800: 1.578648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 4900: 1.601009 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 5000: 1.622128 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "nkoins inters which to but is the three five six zero three six six four assacro \n",
      "me althought it composimority and was kenning ly justriations for the inly extens\n",
      "kb a spoppeatick activity are vants of two by minotral seme chaked bek to kess th\n",
      "wfied that mans jecter link in the namible that impsys about close we entrom of s\n",
      "zg lam turns of the anizafution came are secondinadding four three justitarus ste\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 5100: 1.623965 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 5200: 1.620314 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5300: 1.587249 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 5400: 1.586745 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 5500: 1.569929 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5600: 1.590115 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 5700: 1.554967 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5800: 1.556078 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 5900: 1.578708 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 6000: 1.547386 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "zery flying who infinds lead such content intal to dailin game to the codes thus \n",
      "km abable what garticles invoxie or usa reacommer thestal range for terr produced\n",
      "tleted hel on of terraniana pubritic city in havy four demooses he with some actu\n",
      "mrise regardes are in in anamore and is sinmens also screation has ball posis ant\n",
      "osonaling in the spened boy worza it in the raighout stoccuuun and dory of politi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 6100: 1.562557 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 6200: 1.589644 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 6300: 1.599117 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 6400: 1.630508 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 6500: 1.625223 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 6600: 1.582736 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 6700: 1.576683 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 6800: 1.561003 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 6900: 1.552204 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 7000: 1.567820 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "================================================================================\n",
      "chation to the encine with and in pwcrip c so advanct deet instruct ext can skill\n",
      "heald mas add hels two five two zero zero sdately general can was economip inted \n",
      "smed lines covered thenolog to which showwads of near of pring late or notive a m\n",
      "dp as a requerist won geoglace was  lisu s kill wage during er the attocombed spe\n",
      "er connect bus of an and not had a for choide to who exceper shumel peappeared an\n",
      "================================================================================\n",
      "Validation set perplexity: 6.76\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works, but not ideal - perplexity is a bit higher. Now adding a dropout. Proper way of doing it is ensure it is only for input or output weights, not anywhere in cells. We also give it more time since dropout slows down the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.307707 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.32\n",
      "================================================================================\n",
      "yq   jodw  ekbi pbllpa mnhz trnetgfqbef otm koip tutg fzkm ljwifw   d kssoewag ex\n",
      "pcxul xeiezht ididurdtwyoglniiven zsnj shz ruwkwtxniidq nyit ugctndefenc      qrc\n",
      "fbwmen b eofyylmerohufnhj wvnmahv tcxim   ilo vli iqtirti eyxsvel jc  rtnqudiqaea\n",
      "gegca   c q mnytfeiozfwo nnao d isr no mb   q   scc wryqlpea   lnptlpq eaju   eot\n",
      "atl onps oaapdvplhuetn a marbriiogfcpitpcei g zcf fnag b gozfwfjdeetmya trkrimd z\n",
      "================================================================================\n",
      "Validation set perplexity: 20.88\n",
      "Average loss at step 100: 2.288428 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 200: 1.972137 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 300: 1.887561 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 400: 1.829656 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 500: 1.763462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 600: 1.759068 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 700: 1.742174 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 800: 1.722131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 900: 1.717509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 1000: 1.684483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "knowled not others bobkowed is which pelone png of foot the metremplelegen shier \n",
      " x lin commarken evaction in he commencies in the s mulutions for botry one six m\n",
      "jzurea conversions wilgoing markely long lection followed be corder use for retit\n",
      "dy eured were are metaimses suppose medift davics peted clowns pena that sommed a\n",
      "bet erception wine cesconcumdy bool mounk the the essens amount speign in betreni\n",
      "================================================================================\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1100: 1.690865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 1200: 1.689439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 1300: 1.690924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 1400: 1.663637 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 1500: 1.645893 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1600: 1.643712 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1700: 1.646513 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1800: 1.665361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 1900: 1.650776 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2000: 1.658540 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "================================================================================\n",
      "zve one nine nine seven the milliew sently cillege doluguard archiph mowevel perf\n",
      "mplies inter war no appear and ge one seven zero one mzero as on the distime thr \n",
      "rban informal culture infinaly cloth unial cd helped on the include enerven first\n",
      "fs complement high new politifor all plase many of and roman apanalyla amligi chi\n",
      "qeen olvely to his oppen ira tradition north under a over asiskles set it fore ab\n",
      "================================================================================\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2100: 1.643199 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 2200: 1.664311 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2300: 1.646305 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 2400: 1.646259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 2500: 1.653047 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 2600: 1.643215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2700: 1.624146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2800: 1.626631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 2900: 1.622738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 3000: 1.642750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "bkel depents eneth isionali  if the scontos rate schools critish been one seven p\n",
      " continued and jury be fidisk s toversigne flynns in uncrics in lized in heaving \n",
      "it the treks to the soughj sines comercistose of members of elismmune rich by coi\n",
      "kumbers and marring all werricohics are as achufferes screen criticism colore a r\n",
      "eubs veriots ne small can in butter toustabli secent of laboleed in gover base ca\n",
      "================================================================================\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 3100: 1.612641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3200: 1.628608 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 3300: 1.624235 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 3400: 1.626339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 3500: 1.606242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 3600: 1.626583 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 3700: 1.594421 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 3800: 1.600126 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3900: 1.583246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 4000: 1.605594 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "vbm from estober boola scorded the clockettrances have a forces inflaws and the c\n",
      " known into the garaded what kowns and one nine nine six six to s are grasm islan\n",
      "ield bokespanst a orecation the death to for one nine nine  one five nine nine th\n",
      "i holuteism off of singebics and sugpbehurnist mwsrian be nga thistence from syst\n",
      "he eouse flacked publiii rasymbinel declargord new gibraltart sayes andameorgitio\n",
      "================================================================================\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 4100: 1.620801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 4200: 1.603131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 4300: 1.565339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 4400: 1.594917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 4500: 1.582593 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 4600: 1.582591 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 4700: 1.599122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 4800: 1.591842 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 4900: 1.618318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 5000: 1.622085 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "lato  s partilio were of the period that s pecamber four five zero zero bher gall\n",
      "fbs of data and tite of here of anims american on the naxross who a religion of a\n",
      "ologatic cases the for the soverzst one nine four ension in the protection other \n",
      "ed in mainst the rose which be restalists barcher mac islands ood internation on \n",
      "shinems was in the humans spart which les year of the pholfm is the our by lectle\n",
      "================================================================================\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 5100: 1.599396 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5200: 1.614842 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 5300: 1.587171 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 5400: 1.587176 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 5500: 1.590808 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 5600: 1.564751 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 5700: 1.600204 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 5800: 1.585767 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 5900: 1.598711 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 6000: 1.555034 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "pb girised their was different immition calcing note pak was a post be bearse fou\n",
      "zhuntage all national deaths and companies abots under is one seven five there al\n",
      " rid bears manufactors cartered is ins id and millife in deemulate ettle name ono\n",
      "duday towing with bennium betmps to grouplayers areounting kcwued a anot acceptos\n",
      "oheaday by und what are to sr died and bel was lined average the queub of is ephe\n",
      "================================================================================\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6100: 1.609398 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 6200: 1.607213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 6300: 1.591901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 6400: 1.606745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 6500: 1.604443 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 6600: 1.594386 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 6700: 1.589683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 6800: 1.601033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 6900: 1.632819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 7000: 1.615375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "bwer sufficient tipen in cance and in the westiminal starting then or that intine\n",
      "cx for ard and calawar a nevery are and there on bvery of his freleade wakaol ogr\n",
      "wzalk intennable the rubise b one seven is but war electly madard to major tries \n",
      "xplize v the unplay of intest from to km pomains aoy in intents war simeteen tiel\n",
      "wdelleo aftic entitutes four the uootter novele or jestribution one ebect aimsezt\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 7100: 1.597276 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 7200: 1.584039 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 7300: 1.589337 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 7400: 1.605368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 7500: 1.602366 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 7600: 1.581648 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 7700: 1.598986 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 7800: 1.572836 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 7900: 1.591467 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 8000: 1.587697 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "dy in s normanes dooks of was suscial largest everine inficial empe three two one\n",
      "yylo be after egas litte mr othelre crues without berth cam scusset an one punp w\n",
      "mcs classible dkinam of the caneous of the equivale the united with more whop pla\n",
      "mh prewards the chine ws by tancting battle chemical decitistian landreasons west\n",
      "jh only adasing nobeliefre all elcines silly exue be punginning statem he a main \n",
      "================================================================================\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 8100: 1.586117 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 8200: 1.584901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 8300: 1.578353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 8400: 1.586298 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 8500: 1.605946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 8600: 1.600523 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 8700: 1.580766 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 8800: 1.618577 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 8900: 1.619255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 9000: 1.580674 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "wxevieth american with the purpodule ican on feir objection high a unclusion who \n",
      "qxo frongintig and smell explomereg that room casaddings also the lified by requi\n",
      " defnnent husgby he are paces hapfer engines designy five nine zero zero zero ess\n",
      "rzing two he count smieces privat of at prommon influs for port if zlenz mancade \n",
      "only on crimmand one one two six one nine one zero one one known art projectione \n",
      "================================================================================\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 9100: 1.585336 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 9200: 1.573742 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 9300: 1.615688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 9400: 1.604908 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 9500: 1.573849 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 9600: 1.577072 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 9700: 1.575489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 9800: 1.580882 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 9900: 1.566601 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 10000: 1.562257 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      " x raletts who navian subsponua entrate british circlionally two three seven fast\n",
      " programming of centricted the manage of palaces in kinduguado bnity of smututant\n",
      "d thus calticulating at arts would dary ventury is its of starrient of this andom\n",
      "ez ird pedearceply witting tition producism don the named two four ristries of co\n",
      "fina b has the was acaking nontiallyrring cubet the control speakfalgeagues sayin\n",
      "================================================================================\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 10100: 1.591213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 10200: 1.592907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 10300: 1.582871 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 10400: 1.580337 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 10500: 1.568722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 10600: 1.547997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 10700: 1.563076 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 10800: 1.577623 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 10900: 1.595626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 11000: 1.570978 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "uqal are originapanism final have perred mtten art is the areas story pay to a te\n",
      "amost within while this hypekfigured to in and hands uses laternary any scanence \n",
      "yvote swas in the project imply s had has that four seven apbated was do house ri\n",
      "vtance and under one nine one nine an espeed s chronicoliddits and one nine jews \n",
      "es of mobilication revises historocler nation of distal assemblys palace stated a\n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 11100: 1.584442 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 11200: 1.565821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 11300: 1.567547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 11400: 1.578805 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 11500: 1.577772 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 11600: 1.571665 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 11700: 1.566860 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 11800: 1.588202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 11900: 1.572198 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 12000: 1.591125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "yfull into her as film scheection and thusage ar constituting nami refer anden fr\n",
      "tvisii peelpine planeal single considerfetrials this paid is other the year inter\n",
      "dward just in the paaassidence series sor overnation illves that formed zoness in\n",
      "lkless that became from will with numbers deligium their shot of grandes with and\n",
      " x impwricks berthward formed is qually intertnimicy teams and two five if reseat\n",
      "================================================================================\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 12100: 1.582978 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 12200: 1.581617 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 12300: 1.578689 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 12400: 1.561232 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 12500: 1.565497 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 12600: 1.590277 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 12700: 1.569656 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 12800: 1.565570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 12900: 1.575351 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 13000: 1.579016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "tx two one nine zero one five largese flagmorad but are afteumaging and is gation\n",
      "fer during drawn arabok mixrine the disher dava enyip is of star nations in two c\n",
      "sverally remained position az of and first ejectors sex an obsonicalling them pga\n",
      "oge re munition one zero zero plot his not or as magdomaria edge one nine seven o\n",
      "of bond bamen document franculatively artics and egyuced arounia clusion word blo\n",
      "================================================================================\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 13100: 1.598098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 13200: 1.578728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 13300: 1.590470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 13400: 1.609697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 13500: 1.628334 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 13600: 1.598528 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 13700: 1.596533 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 13800: 1.582360 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 13900: 1.553774 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 14000: 1.592909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "bv on the gential openburing the from one zero zero human annrown emisters that a\n",
      "ct prirms temperazi cold not raughte and one zero zero zero zero zero five two ze\n",
      "mnes ann group nermed and deep even the was paniament functions becemac case from\n",
      " of pah of resourges bases taizing only defirsable in one km at tru years to pont\n",
      "tgound in a medleadst during as uvture was the tains off its from mygood it was l\n",
      "================================================================================\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 14100: 1.584697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 14200: 1.582200 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 14300: 1.590037 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 14400: 1.583328 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 14500: 1.580060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 14600: 1.565338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 14700: 1.582770 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 14800: 1.601414 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 14900: 1.607920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 15000: 1.591738 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "hul sanded begable while copn togiance interess thistoon berhoradial vor of viule\n",
      "fvlas for german mm estitutes links one zero zero zero zero zero five form one fo\n",
      "mv refer collection of suffered known in famous one five one horiest or the conal\n",
      "ymon iriscus carchle topoyed algol image approtetium bicgmbanding dated by canthe\n",
      "wxels of used a prestereded to dard learchlywously ext invents many as reports cu\n",
      "================================================================================\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 15100: 1.556326 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 15200: 1.559679 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 15300: 1.575617 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 15400: 1.569695 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 15500: 1.577246 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 15600: 1.574626 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 15700: 1.563387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 15800: 1.572512 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 15900: 1.551194 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 16000: 1.560699 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "loutical won socially reference locked and the d of a vition all intervation not \n",
      "gineerame chrome atch cumbes various for molection drartments dc occuportatter pa\n",
      "rgebity in furkners of mathematicative norally fmr the word stages becotha claim \n",
      "mt smovel ain dimax a some theat the number there are of their tobsere iden strai\n",
      "yknoicing one scottic to was public of tribed it wars helse visators of him zo fr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 16100: 1.569832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 16200: 1.587797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 16300: 1.573118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 16400: 1.556463 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 16500: 1.568952 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16600: 1.561494 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 16700: 1.571820 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 16800: 1.571383 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16900: 1.536654 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 17000: 1.555611 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "rganization of also a symbo it as into revolution s new grank issuisting thingnes\n",
      "md crete example groattle thomaning one eight zereom format which riouse was to b\n",
      "r a served by digia taking html as six type basex using as french appeudel early \n",
      "of inter speciality reading of frequent years who  station had shop to changived \n",
      " rategain one zero killutes the montheer to the finammic definition and piece in \n",
      "================================================================================\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 17100: 1.567158 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17200: 1.557044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 17300: 1.548753 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 17400: 1.564187 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 17500: 1.579266 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 17600: 1.524399 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17700: 1.535340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17800: 1.552381 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 17900: 1.562207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 18000: 1.528207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "harns eishes cographals execulture years is and ligh an and seen should incifient\n",
      "ies either several signational the milablished by redefine and ward someterbut th\n",
      "cusing calating to indinalifarting and language cology only of moleches this the \n",
      "tqns evalu such original in matmends democroplus bppeared the vailitial one nine \n",
      "cf though discention of a several or experient accown fracting zocculting so marr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18100: 1.513316 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 18200: 1.515940 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 18300: 1.508244 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18400: 1.518304 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 18500: 1.520146 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 18600: 1.498316 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 18700: 1.495401 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 18800: 1.519591 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 18900: 1.507814 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 19000: 1.489757 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "bzinligag pusion and criminging was hiclineuor projccation the feature of succeas\n",
      "snory show moving the questipace of rembouny of the during the travs keep program\n",
      "kwished nevera s the positured in one countired the header medible and immunitied\n",
      "axic shormouns of the enate damage theory of number   does and greatly c a fortly\n",
      "of scengobe no raised document portance and calleds in all atters raphy and all i\n",
      "================================================================================\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 19100: 1.489248 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 19200: 1.515504 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 19300: 1.505923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 19400: 1.546479 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 19500: 1.512875 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 19600: 1.498232 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 19700: 1.507923 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.79\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 19800: 1.521222 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 19900: 1.547086 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 20000: 1.515441 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "x the one nine one nine two sable by fought someton to dralumbial to are nove ale\n",
      "ryth the species of and with founded time so storistdany on is class end precedes\n",
      "jor with the which mywas of the wicles and and is the from achiefor as statu masc\n",
      "xwt and result mikhihaustries for itsan in the one nine one nine eight femaliting\n",
      "npomemon provsminaor there active posititiscomplear in which three one nine eight\n",
      "================================================================================\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 20100: 1.516845 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 20200: 1.556226 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 20300: 1.539660 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 20400: 1.537646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 20500: 1.549592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 20600: 1.534634 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 20700: 1.507485 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 20800: 1.490941 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 20900: 1.511634 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 21000: 1.516525 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "lve fries or a s written of iii heel the language barter plarginel usually were p\n",
      "of his changed in common the koren their reduces who one nine seven seven three a\n",
      "xschot scronect the china augaerom party of levision an etc one six four nine and\n",
      "as rews not decond tentland knears mermanoboth the one zero zero respon modern pa\n",
      "when time the its similar avery elligent in commov accessionished in menial of sy\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
